{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b6fd2-cd67-444a-92e6-ab63d3ee0f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt update\n",
    "!apt install -y libgl1-mesa-glx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e1a40-7d3e-49a2-a4de-93032c56a5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setupModel.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55549c85-8205-46ee-81a7-88077bce1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_dict=get_images_from_s3(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0002af-e4b2-4431-87e8-5e36982859d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params():\n",
    "    # All configurable parameters\n",
    "    return {\n",
    "        'threshold_minutes': 15,\n",
    "        'ghi_resolution': '15m',\n",
    "        'int_ghi_resolution': 15,\n",
    "        'num_previous_images': 2,\n",
    "        'Hdelta_t': 16,\n",
    "        'delta_t': 15,  # Be careful: multiplied by image frequency\n",
    "        'image_delta_t': 1\n",
    "    }\n",
    "# Call the params function and unpack values into variables\n",
    "config = params()\n",
    "threshold_minutes = config['threshold_minutes']\n",
    "ghi_resolution = config['ghi_resolution']\n",
    "int_ghi_resolution = config['int_ghi_resolution']\n",
    "num_previous_images = config['num_previous_images']\n",
    "Hdelta_t = config['Hdelta_t']\n",
    "delta_t = config['delta_t']\n",
    "image_delta_t = config['image_delta_t']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c885d168-c955-4800-b3e9-510dd293d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keys = get_images_keys_from_s3(images_every) # List of image keys\n",
    "image_keys = list(image_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd41f42-62a2-4d83-a516-854b11323851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InfluxDBQueryClient:\n",
    "    def __init__(self, ip, port, token, org):\n",
    "        self.url = f\"http://{ip}:{port}\"\n",
    "        self.token = token\n",
    "        self.org = org\n",
    "        self.timeout=5e5\n",
    "        self.client = InfluxDBClient(url=self.url, token=self.token, org=self.org,timeout=self.timeout)\n",
    "\n",
    "    def query_measured_ghi(self, start_time, end_time, resolution):\n",
    "        \"\"\"\n",
    "        Query the measured GHI from the InfluxDB database\n",
    "        :param start_time: datetime.datetime\n",
    "        :param end_time: datetime.datetime\n",
    "        :param resolution: str\n",
    "        :return: pd.DataFrame\n",
    "        \"\"\"\n",
    "        bucket = \"microgrid_ST\"\n",
    "\n",
    "        # Convert datetime to Unix timestamp\n",
    "        t0 = round(start_time.timestamp())\n",
    "        tf = round(end_time.timestamp())\n",
    "\n",
    "        # Prepare the Flux query\n",
    "        flux_query = f\"\"\"\n",
    "        from(bucket: \"microgrid_ST\")\n",
    "        |> range(start: {t0}, stop: {tf})\n",
    "          |> filter(fn: (r) => r[\"_measurement\"] == \"microgrid\")\n",
    "          |> filter(fn: (r) => r[\"Resource\"] == \"meteobox_roof\")\n",
    "          |> filter(fn: (r) => r[\"_field\"] == \"GHI\")\n",
    "          |> aggregateWindow(every: {resolution}, fn: mean, createEmpty: false)\n",
    "          |> yield(name: \"mean\")\n",
    "          |> pivot(rowKey:[\"_time\"], columnKey: [\"Resource\"], valueColumn: \"_value\")\n",
    "        \"\"\"\n",
    "\n",
    "        # Query the data\n",
    "        query_api = self.client.query_api()\n",
    "        measured_GHI_api15 = query_api.query_data_frame(org=self.org, query=flux_query)\n",
    "        return measured_GHI_api15[0]\n",
    "\n",
    "    \n",
    "    def pull_solcast_forecast(self, forecast_time):\n",
    "        \"\"\"\n",
    "        Pull Solcast forecast data for GHI from InfluxDB.\n",
    "        :param forecast_time: datetime.datetime, start of the query range\n",
    "        :return: pd.DataFrame\n",
    "        \"\"\"\n",
    "        to = forecast_time + timedelta(minutes=1)\n",
    "        # Convert datetime to Unix timestamp\n",
    "        t0 = round(forecast_time.timestamp())\n",
    "        tf = round(to.timestamp())\n",
    "\n",
    "        bucket = \"Forecasting_ST\"\n",
    "\n",
    "        # Prepare the Flux query\n",
    "        flux_query = f\"\"\"\n",
    "        from(bucket: \"{bucket}\")\n",
    "        |> range(start: {t0}, stop: {tf})\n",
    "        |> filter(fn: (r) => r[\"type\"] == \"Solcast\")\n",
    "        |> filter(fn: (r) => r[\"_field\"] == \"ghi\")\n",
    "        |> pivot(rowKey:[\"_time\"], columnKey: [\"prediction_time\"], valueColumn: \"_value\")\n",
    "        |> yield(name: \"mean\")\n",
    "        \"\"\"\n",
    "        # Query the data\n",
    "        query_api = self.client.query_api()\n",
    "        solcast_forecast = query_api.query_data_frame(org=self.org, query=flux_query)\n",
    "\n",
    "        return solcast_forecast\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    def pull_solcast_forecast_bulk(self, start_time, end_time):\n",
    "        \"\"\"\n",
    "        Pull Solcast forecast data for GHI from InfluxDB over a time range.\n",
    "        :param start_time: datetime.datetime\n",
    "        :param end_time: datetime.datetime\n",
    "        :return: pd.DataFrame\n",
    "        \"\"\"\n",
    "        bucket = \"Forecasting_ST\"\n",
    "        t0 = round(start_time.timestamp())\n",
    "        tf = round(end_time.timestamp())\n",
    "    \n",
    "        flux_query = f\"\"\"\n",
    "        from(bucket: \"{bucket}\")\n",
    "        |> range(start: {t0}, stop: {tf})\n",
    "        |> filter(fn: (r) => r[\"type\"] == \"Solcast\")\n",
    "        |> filter(fn: (r) => r[\"_field\"] == \"ghi\")\n",
    "        |> pivot(rowKey:[\"_time\"], columnKey: [\"prediction_time\"], valueColumn: \"_value\")\n",
    "        |> yield(name: \"mean\")\n",
    "        \"\"\"\n",
    "    \n",
    "        query_api = self.client.query_api()\n",
    "        try:\n",
    "            df = query_api.query_data_frame(org=self.org, query=flux_query)\n",
    "        except Exception as e:\n",
    "            print(\"Error al hacer la consulta:\", e)\n",
    "            raise\n",
    "\n",
    "        #df = query_api.query_data_frame(org=self.org, query=flux_query)\n",
    "        return df\n",
    "    def pull_solcast_forecast_bulk2(self, start_time, end_time):\n",
    "        bucket = \"Forecasting_ST\"\n",
    "        t0 = round(start_time.timestamp())\n",
    "        tf = round(end_time.timestamp())\n",
    "    \n",
    "        flux_query = f\"\"\"\n",
    "        from(bucket: \"{bucket}\")\n",
    "        |> range(start: {t0}, stop: {tf})\n",
    "        |> filter(fn: (r) => r[\"type\"] == \"Solcast\")\n",
    "        |> filter(fn: (r) => r[\"_field\"] == \"ghi\" or r[\"_field\"] == \"ghi10\" or r[\"_field\"] == \"ghi90\")\n",
    "        |> pivot(rowKey:[\"_time\"], columnKey: [\"prediction_time\", \"_field\"], valueColumn: \"_value\")\n",
    "        |> yield(name: \"mean\")\n",
    "        \"\"\"\n",
    "    \n",
    "        query_api = self.client.query_api()\n",
    "        try:\n",
    "            df = query_api.query_data_frame(org=self.org, query=flux_query)\n",
    "        except Exception as e:\n",
    "            print(\"Error al hacer la consulta:\", e)\n",
    "            raise\n",
    "    \n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296e35a5-6710-4241-a9e5-9e46db3420c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_merged_data_dict(image_keys,\n",
    "                            threshold_minutes=15,\n",
    "                            num_previous_images=3,\n",
    "                            image_delta_t=1,\n",
    "                            Hdelta_t=3,\n",
    "                            delta_t=1):\n",
    "    start_total = time.time()\n",
    "\n",
    "    # --- 1) Prepare image_df ---\n",
    "    t1 = time.time()\n",
    "    image_timestamps = [extract_timestamp_from_image(k) for k in image_keys]\n",
    "    image_df = pd.DataFrame({\n",
    "        'image_timestamp': pd.to_datetime(image_timestamps),\n",
    "        'image_key': image_keys\n",
    "    }).sort_values('image_timestamp').reset_index(drop=True)\n",
    "    image_df['img_idx'] = image_df.index\n",
    "    print(f\"Step 1 (Prepare image_df) took {time.time() - t1:.2f}s\")\n",
    "\n",
    "    # --- 2) Time range for GHI query ---\n",
    "    t2 = time.time()\n",
    "    dates = image_df['image_timestamp'].dt.date.unique()\n",
    "    starts = [pd.Timestamp(d).tz_localize('UTC') - pd.Timedelta(hours=4) for d in dates]\n",
    "    ends   = [pd.Timestamp(d).tz_localize('UTC') + pd.Timedelta(days=1, hours=4) for d in dates]\n",
    "    start_time, end_time = min(starts), max(ends) + pd.Timedelta(minutes=1)\n",
    "    print(f\"Step 2 (Compute query window) took {time.time() - t2:.2f}s\")\n",
    "\n",
    "    # --- 3) Query GHI from InfluxDB ---\n",
    "    t3 = time.time()\n",
    "    client = InfluxDBQueryClient(\n",
    "        ip='***',\n",
    "        port=***,\n",
    "        token='***',\n",
    "        org=\"DESL-EPFL\",\n",
    "    )\n",
    "    \n",
    "    ghi_df = client.query_measured_ghi(start_time, end_time, '1m')\n",
    "         \n",
    "     \n",
    "    ghi_df = (ghi_df.rename(columns={'_time':'time','_value':'GHI'})\n",
    "                  .assign(time=lambda df: pd.to_datetime(df['time']).dt.tz_convert('UTC'))\n",
    "                  .set_index('time'))\n",
    "    ghi_df['GHI_15min_avg'] = ghi_df['GHI'].rolling(window=15, center=True, min_periods=1).mean()\n",
    "    print(f\"Step 3 (Query GHI) took {time.time() - t3:.2f}s\")\n",
    "\n",
    "    # --- 4) Clear-sky calculation ---\n",
    "    t4 = time.time()\n",
    "    time_range = pd.date_range(start_time, end_time, freq='1min', tz='UTC')\n",
    "    cs = batiment.get_clearsky(time_range).ghi.rename('clear_sky_GHI').to_frame()\n",
    "    print(f\"Step 4 (Clear-sky calc) took {time.time() - t4:.2f}s\")\n",
    "\n",
    "    # --- 5) Merge asof measured GHI at image times ---\n",
    "    t5 = time.time()\n",
    "    merged = pd.merge_asof(\n",
    "        image_df,\n",
    "        ghi_df[['GHI','GHI_15min_avg']].reset_index().rename(columns={'time':'closest_time'}),\n",
    "        left_on='image_timestamp', right_on='closest_time',\n",
    "        direction='nearest', tolerance=pd.Timedelta(minutes=threshold_minutes)\n",
    "    )\n",
    "    merged['time_diff'] = (merged['image_timestamp'] - merged['closest_time']).abs().dt.total_seconds()/60\n",
    "    merged.loc[merged['time_diff']>threshold_minutes, ['GHI','GHI_15min_avg']] = np.nan\n",
    "    merged = merged.dropna(subset=['GHI'])\n",
    "    print(f\"Step 5 (As-of merge GHI) took {time.time() - t5:.2f}s\")\n",
    "\n",
    "    # --- 6) Merge asof clear-sky at image times ---\n",
    "    t6 = time.time()\n",
    "    merged = pd.merge_asof(\n",
    "        merged.sort_values('image_timestamp'),\n",
    "        cs.reset_index().rename(columns={'index':'closest_time'}),\n",
    "        on='closest_time', direction='nearest', tolerance=pd.Timedelta(minutes=threshold_minutes)\n",
    "    ).dropna(subset=['clear_sky_GHI'])\n",
    "    print(f\"Step 6 (As-of merge clear-sky) took {time.time() - t6:.2f}s\")\n",
    "\n",
    "    # --- 7) Daily max clear-sky ---\n",
    "    t7 = time.time()\n",
    "    merged['date'] = merged['image_timestamp'].dt.date\n",
    "    merged['max_clear_sky_ghi'] = merged.groupby('date')['clear_sky_GHI'].transform('max')\n",
    "    print(f\"Step 7 (Max clear-sky) took {time.time() - t7:.2f}s\")\n",
    "\n",
    "    # --- 8) Normalize ---\n",
    "    t8 = time.time()\n",
    "    merged['GHI_normalized'] = merged['GHI'] / merged['max_clear_sky_ghi']\n",
    "    merged['clear_sky_ghi_normalized'] = merged['clear_sky_GHI'] / merged['max_clear_sky_ghi']\n",
    "    print(f\"Step 8 (Normalization) took {time.time() - t8:.2f}s\")\n",
    "\n",
    "    # --- 9) Past images list ---\n",
    "    t9 = time.time()\n",
    "    for i in range(1, num_previous_images+1):\n",
    "        merged[f'prev_img_{i}'] = merged['image_key'].shift(i * image_delta_t)\n",
    "    merged['image_keys'] = merged.apply(\n",
    "        lambda r: [r['image_key']] + [r[f'prev_img_{i}'] for i in range(1, num_previous_images+1) if pd.notna(r[f'prev_img_{i}'])], axis=1\n",
    "    )\n",
    "    print(f\"Step 9 (Past images) took {time.time() - t9:.2f}s\")\n",
    "\n",
    "    # --- 10) Build future-offset table ---\n",
    "    t10 = time.time()\n",
    "    img = merged[['img_idx','image_timestamp','max_clear_sky_ghi']]\n",
    "    steps = np.arange(0, Hdelta_t+1) * delta_t\n",
    "    offsets = pd.DataFrame({\n",
    "        'img_idx': np.repeat(img['img_idx'].values, len(steps)),\n",
    "        'step': np.tile(steps, len(img))\n",
    "    })\n",
    "    offsets['forecast_time'] = offsets['img_idx'].map(img.set_index('img_idx')['image_timestamp']) + pd.to_timedelta(offsets['step'], unit='m')\n",
    "    print(f\"Step 10 (Build offsets) took {time.time() - t10:.2f}s\")\n",
    "\n",
    "    # --- 11) Bulk asof for GHI and clear-sky at offsets ---\n",
    "    t11 = time.time()\n",
    "    ghi_lu = ghi_df.reset_index()[['time','GHI']]\n",
    "    cs_lu  = cs.reset_index().rename(columns={'index':'time'})[['time','clear_sky_GHI']]\n",
    "    offsets = pd.merge_asof(offsets.sort_values('forecast_time'), ghi_lu.sort_values('time'), left_on='forecast_time', right_on='time', direction='nearest', tolerance=pd.Timedelta(minutes=threshold_minutes))\n",
    "    offsets = pd.merge_asof(offsets.sort_values('forecast_time'), cs_lu.sort_values('time'), left_on='forecast_time', right_on='time', direction='nearest', tolerance=pd.Timedelta(minutes=threshold_minutes))\n",
    "    print(f\"Step 11 (Bulk asof) took {time.time() - t11:.2f}s\")\n",
    "\n",
    "    # --- 12) Group into sequences ---\n",
    "    t12 = time.time()\n",
    "    seqs = (offsets.sort_values(['img_idx','step'])\n",
    "            .groupby('img_idx')\n",
    "            .agg({'GHI': lambda x: x.tolist(), 'clear_sky_GHI': lambda x: x.tolist()})\n",
    "           )\n",
    "    seqs['max_cs'] = img.set_index('img_idx')['max_clear_sky_ghi']\n",
    "    seqs['ghi_values_normalized'] = seqs.apply(lambda r: [v/r['max_cs'] if pd.notna(v) else np.nan for v in r['GHI']], axis=1)\n",
    "    seqs['clear_sky_ghi_normalized_seq'] = seqs.apply(lambda r: [v/r['max_cs'] if pd.notna(v) else np.nan for v in r['clear_sky_GHI']], axis=1)\n",
    "    seqs = seqs.rename(columns={'GHI':'ghi_values','clear_sky_GHI':'clear_sky_ghi_seq'})\n",
    "    print(f\"Step 12 (Group sequences) took {time.time() - t12:.2f}s\")\n",
    "\n",
    "    # --- 13) Join sequences back and build dict ---\n",
    "    t13 = time.time()\n",
    "    merged = merged.join(seqs, on='img_idx')\n",
    "    print(f\"Step 13 (Join sequences) took {time.time() - t13:.2f}s\")\n",
    "\n",
    "    t14 = time.time()\n",
    "    records = merged.dropna(subset=['ghi_values'])[[\n",
    "        'image_timestamp','image_key','ghi_values','ghi_values_normalized',\n",
    "        'clear_sky_ghi_seq','clear_sky_ghi_normalized_seq','max_clear_sky_ghi','image_keys'\n",
    "    ]].to_dict(orient='records')\n",
    "\n",
    "    merged_data_dict = {\n",
    "        rec['image_timestamp']: dict(\n",
    "            image_key=rec['image_key'],\n",
    "            ghi_values=rec['ghi_values'],\n",
    "            ghi_values_normalized=rec['ghi_values_normalized'],\n",
    "            clear_sky_ghi_seq=rec['clear_sky_ghi_seq'],\n",
    "            clear_sky_ghi_normalized_seq=rec['clear_sky_ghi_normalized_seq'],\n",
    "            max_clear_sky_ghi=rec['max_clear_sky_ghi'],\n",
    "            image_keys=rec['image_keys'],\n",
    "            key_timestamp=rec['image_timestamp']\n",
    "        ) for rec in records\n",
    "    }\n",
    "    print(f\"Step 14 (Build dict) took {time.time() - t14:.2f}s; Records: {len(merged_data_dict)}\")\n",
    "\n",
    "        # --- 15) Solcast: per-key latest forecast ---\n",
    "    t15 = time.time()\n",
    "    x_splits=5\n",
    "    # 1) Compute overall time window\n",
    "    t_start = min(v['key_timestamp'] for v in merged_data_dict.values()) - timedelta(minutes=10)\n",
    "    t_end   = max(v['key_timestamp'] for v in merged_data_dict.values()) + timedelta(minutes=10)\n",
    "    \n",
    "    # 2) Generate split boundaries\n",
    "    total_duration = t_end - t_start\n",
    "    splits = [\n",
    "        t_start + i * (total_duration / x_splits)\n",
    "        for i in range(x_splits + 1)\n",
    "    ]\n",
    "    \n",
    "    # 3) Pull each chunk\n",
    "    dfs = []\n",
    "    for i in range(x_splits):\n",
    "        start_i = splits[i]\n",
    "        end_i   = splits[i+1]\n",
    "        t0 = time.time()\n",
    "        df_i = client.pull_solcast_forecast_bulk2(start_i, end_i)\n",
    "        print(f\"Pull {i+1}/{x_splits} finished in {time.time() - t0:.2f} s\")\n",
    "        dfs.append(df_i)\n",
    "    \n",
    "    # 4) Combine, parse and index\n",
    "    sol_df = pd.concat(dfs, axis=0)\n",
    "    sol_df['_time'] = pd.to_datetime(sol_df['_time'])\n",
    "    sol_df = sol_df.drop_duplicates(subset='_time').set_index('_time').sort_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"Step 15.1 query (Solcast) took {time.time() - t15:.2f}s\")\n",
    "    t15 = time.time()\n",
    "    lead_minutes = (np.arange(1, Hdelta_t+1) * 15.0).tolist()\n",
    "    mean_cols = [f\"{m}_ghi\" for m in lead_minutes]\n",
    "    q10_cols  = [f\"{m}_ghi10\" for m in lead_minutes]\n",
    "    q90_cols  = [f\"{m}_ghi90\" for m in lead_minutes]\n",
    "\n",
    "    # For each timestamp, find the last Solcast row at or before that time\n",
    "    sol_index = sol_df.index\n",
    "    for ts, rec in merged_data_dict.items():\n",
    "        pos = sol_index.searchsorted(ts) - 1  # index of last row <= ts\n",
    "        max_clearsky = rec.get('max_clear_sky_ghi', np.nan)\n",
    "        if pos < 0:\n",
    "            # no forecast available before ts\n",
    "            preds = np.full((3, Hdelta_t), np.nan)\n",
    "        else:\n",
    "            row = sol_df.iloc[pos]\n",
    "            # extract values\n",
    "            q10_vals  = row[q10_cols].values.astype(float)/max_clearsky\n",
    "            mean_vals = row[mean_cols].values.astype(float)/max_clearsky\n",
    "            q90_vals  = row[q90_cols].values.astype(float)/max_clearsky\n",
    "            preds = np.vstack([q10_vals, mean_vals, q90_vals])  # Shape: (3, Hdelta_t)\n",
    "        rec['solcast_predictions'] = preds.tolist()\n",
    "    print(f\"Step 15 (Solcast) took {time.time() - t15:.2f}s\")\n",
    "\n",
    "    # --- 16) Re-index and clean ---\n",
    "    t16 = time.time()\n",
    "    new_dict = {i: v for i, v in enumerate(merged_data_dict.values())}\n",
    "    def has_nan(x):\n",
    "        if isinstance(x, (int, float)):\n",
    "            return np.isnan(x)\n",
    "        if isinstance(x, (list, np.ndarray)):\n",
    "            return any(has_nan(elem) for elem in x)\n",
    "        return False\n",
    "    cleaned = {k: v for k, v in new_dict.items() if not any(has_nan(val) for val in v.values())}\n",
    "    print(f\"Total execution time: {time.time() - start_total:.2f}s; Dropped {len(new_dict)-len(cleaned)} items\")\n",
    "    return cleaned\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3751b13e-5259-4b1b-bb03-1beed439717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retries=0\n",
    "max_retries=10\n",
    "wait_seconds=5\n",
    "while True:\n",
    "    try:\n",
    "        merged_data_dict = create_merged_data_dict(\n",
    "            image_keys,\n",
    "            threshold_minutes,\n",
    "            num_previous_images,\n",
    "            image_delta_t,\n",
    "            Hdelta_t,\n",
    "            delta_t\n",
    "        )\n",
    "        print(\"✅ Merged data dict created successfully.\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        retries += 1\n",
    "        print(f\"⚠️ Attempt {retries} failed: {e}\")\n",
    "        if retries >= max_retries:\n",
    "            raise RuntimeError(\"❌ Max retries exceeded while creating merged_data_dict.\")\n",
    "        print(f\"⏳ Retrying in {wait_seconds} seconds...\")\n",
    "        time.sleep(wait_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680fd6e-46a8-414b-b116-696f8788e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solcast_batchloader(keys, merged_data_dict, image_dict, batch_size,\n",
    "                         expected_num_images, Hdelta_t, use_ghi_now):\n",
    "\n",
    "\n",
    "    def process_sample(k):\n",
    "        k = k.numpy()\n",
    "        if isinstance(k, bytes):\n",
    "            k = k.decode('utf-8')\n",
    "        rec = merged_data_dict[k]\n",
    "\n",
    "        # Existing inputs\n",
    "        image_keys = rec['image_keys'][:expected_num_images]\n",
    "        image_seq = np.stack([image_dict[ik] for ik in image_keys], axis=0)\n",
    "        clear_sky_input = np.array(rec['clear_sky_ghi_normalized_seq'][:Hdelta_t + 1])\n",
    "        ghi_now_input = rec['ghi_values_normalized'][0] if use_ghi_now else 0.0\n",
    "\n",
    "        # New solcast input (3 x Hdelta_t)\n",
    "        solcast_input = np.array(rec['solcast_predictions'], dtype=np.float32)\n",
    "\n",
    "        ghi_target = np.array(rec['ghi_values_normalized'][:Hdelta_t + 1])\n",
    "\n",
    "        return (\n",
    "            image_seq.astype(np.float32),\n",
    "            clear_sky_input.astype(np.float32),\n",
    "            np.float32(ghi_now_input),\n",
    "            solcast_input,\n",
    "            ghi_target.astype(np.float32)\n",
    "        )\n",
    "\n",
    "    def tf_process_sample(k):\n",
    "        image_seq, clear_sky_input, ghi_now_input, solcast_input, ghi_target = tf.py_function(\n",
    "            func=process_sample,\n",
    "            inp=[k],\n",
    "            Tout=[tf.float32, tf.float32, tf.float32, tf.float32, tf.float32]\n",
    "        )\n",
    "\n",
    "        # Set shapes\n",
    "        image_seq.set_shape([expected_num_images, 250, 250, 3])\n",
    "        clear_sky_input.set_shape([Hdelta_t + 1])\n",
    "        ghi_now_input.set_shape([])\n",
    "        solcast_input.set_shape([3, Hdelta_t])\n",
    "        ghi_target.set_shape([Hdelta_t + 1])\n",
    "\n",
    "        inputs = {\n",
    "            'image_sequence': image_seq,\n",
    "            'clear_sky_input': clear_sky_input,\n",
    "            'ghi_now_input': tf.expand_dims(ghi_now_input, axis=-1),\n",
    "            'solcast_input': solcast_input\n",
    "        }\n",
    "\n",
    "        return inputs, ghi_target\n",
    "    ds = tf.data.Dataset.from_tensor_slices(np.array(keys, dtype=np.int64))\n",
    "    #ds = tf.data.Dataset.from_tensor_slices(tf.constant(keys, dtype=tf.string))\n",
    "    ds = ds.map(tf_process_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba3505-1267-49d0-a49c-e32c9a593c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_model_quantiles_with_ghi_and_solcast(\n",
    "    num_previous_images,\n",
    "    Hdelta_t,\n",
    "    dropout_rate=0.1,\n",
    "    l2_lambda=1e-5,\n",
    "    lambda_mean_loss=1.0,\n",
    "    lambda_quantile_loss=150.0,\n",
    "    lambda_width_loss=5\n",
    "):\n",
    "    H = Hdelta_t + 1\n",
    "\n",
    "    # --- inputs ---\n",
    "    image_input = layers.Input(shape=(num_previous_images + 1, 250, 250, 3), name='image_sequence')\n",
    "    clear_sky_input = layers.Input(shape=(H,), name='clear_sky_input')\n",
    "    ghi_now_input = layers.Input(shape=(1,), name='ghi_now_input')\n",
    "    solcast_input = layers.Input(shape=(3, Hdelta_t), name='solcast_input')  # [3, Hdelta_t]\n",
    "\n",
    "    # --- deeper 3D-CNN backbone ---\n",
    "    x = layers.Conv3D(32, (3, 3, 3), padding='same', activation='relu')(image_input)\n",
    "    x = layers.Conv3D(32, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling3D((1, 2, 2))(x)\n",
    "\n",
    "    x = layers.Conv3D(64, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv3D(64, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling3D((1, 2, 2))(x)\n",
    "\n",
    "    # --- stacked ConvLSTM2D ---\n",
    "    x = layers.ConvLSTM2D(64, (3, 3), padding='same', return_sequences=True, activation='tanh')(x)\n",
    "    x = layers.ConvLSTM2D(64, (3, 3), padding='same', return_sequences=False, activation='tanh')(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.SpatialDropout2D(dropout_rate)(x)\n",
    "\n",
    "    # --- spatial pooling ---\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # --- deeper clear-sky feature embedding ---\n",
    "    cs = layers.Dense(128, activation='relu')(clear_sky_input)\n",
    "    cs = layers.Dropout(dropout_rate)(cs)\n",
    "    cs = layers.Dense(64, activation='relu')(cs)\n",
    "    cs = layers.Dense(32, activation='relu')(cs)\n",
    "\n",
    "    # --- solcast embedding ---\n",
    "    s = layers.Flatten()(solcast_input)  # flatten [3, Hdelta_t]\n",
    "    s = layers.Dense(64, activation='relu')(s)\n",
    "    s = layers.Dropout(dropout_rate)(s)\n",
    "    s = layers.Dense(32, activation='relu')(s)\n",
    "\n",
    "    # --- concatenate all features ---\n",
    "    x = layers.Concatenate()([x, cs, ghi_now_input, s])\n",
    "\n",
    "    # --- MLP head with residual connection ---\n",
    "    mlp_input = x\n",
    "    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda))(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "    mlp_proj = layers.Dense(128)(mlp_input)  # Project mlp_input to match x\n",
    "    x = layers.Add()([x, mlp_proj])  # Residual connection\n",
    "\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "    # --- output: [H, 3] for [q10, mean, q90] per horizon ---\n",
    "    head = layers.Dense(3 * H, activation='linear')(x)\n",
    "    output = layers.Reshape((H, 3), name='quantiles')(head)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[image_input, clear_sky_input, ghi_now_input, solcast_input],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "    # --- losses and optimizer ---\n",
    "    weighted_mse_metric = weighted_compressed_mse(\n",
    "        H, initial_weight=1.0, second_weight=1.0, decay=0.5\n",
    "    )\n",
    "    weighted_quantile_metric = weighted_quantile_loss(\n",
    "        H, initial_weight=1.0, second_weight=1.0, decay=0.5\n",
    "    )\n",
    "\n",
    "    initial_lr = 3e-4\n",
    "    drop_every_x_steps = 210 * 5\n",
    "    lr_schedule = StepDecay(initial_lr, drop_every_x_steps)\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_schedule)\n",
    "\n",
    "    loss_fn = combined_quantile_loss_factory(\n",
    "        H, lambda_mean=lambda_mean_loss, lambda_q=lambda_quantile_loss, lambda_width=lambda_width_loss\n",
    "    )\n",
    "\n",
    "    def mean_prediction_interval_width(y_true, y_pred):\n",
    "        q10, _, q90 = tf.unstack(y_pred, axis=-1)\n",
    "        width = q90 - q10\n",
    "        return tf.reduce_mean(width)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_fn,\n",
    "        metrics=[\n",
    "            mean_channel_mae,\n",
    "            weighted_mse_metric,\n",
    "            weighted_quantile_metric,\n",
    "            mean_prediction_interval_width\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893116ef-f3c7-487d-8b7e-6a22c9fbc9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f53a4-427e-48e6-ab42-9a50d3b2f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) custom MAE that only looks at the mean channel ---\n",
    "def mean_channel_mae(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_pred[...,1] is the mean prediction.\n",
    "    \"\"\"\n",
    "    mean_pred = y_pred[..., 1]\n",
    "    return tf.reduce_mean(tf.abs(y_true - mean_pred))\n",
    "def weighted_compressed_mse(H, initial_weight=1.0, second_weight=1.0, decay=0.5):\n",
    "    \"\"\"\n",
    "    Returns a metric function that computes a weighted, scaled MSE across horizons.\n",
    "    Weights:\n",
    "      - Horizon 0 → initial_weight\n",
    "      - Horizon i ≥ 1 → second_weight * decay^(i-1)\n",
    "    \"\"\"\n",
    "    def Mean_metric(y_true, y_pred):\n",
    "        mu = y_pred[..., 1]  # extract predicted mean, shape [batch, H]\n",
    "        error = (y_true - mu) * 30\n",
    "        mse_per_horizon = tf.reduce_mean(tf.square(error), axis=0)  # shape [H]\n",
    "\n",
    "        # Compute weights\n",
    "        decay_factors = tf.pow(decay, tf.cast(tf.range(H - 1), tf.float32))\n",
    "        remaining_weights = second_weight * decay_factors\n",
    "        weights = tf.concat([[initial_weight], remaining_weights], axis=0)  # shape [H]\n",
    "\n",
    "        weighted_mse = tf.reduce_sum(weights * mse_per_horizon)\n",
    "        normalization = tf.reduce_sum(weights)\n",
    "        return weighted_mse / normalization\n",
    "\n",
    "    return Mean_metric\n",
    "def weighted_quantile_loss(H, initial_weight=1.0, second_weight=1.0, decay=0.5):\n",
    "    \"\"\"\n",
    "    Returns a metric function that computes the weighted average quantile loss\n",
    "    (pinball loss) over q10 and q90 across horizons.\n",
    "    \"\"\"\n",
    "    def Q_metric(y_true, y_pred):\n",
    "        q10 = y_pred[..., 0]\n",
    "        q90 = y_pred[..., 2]\n",
    "\n",
    "        # Pinball loss per horizon\n",
    "        pin10_h = tf.reduce_mean(quantile_loss(0.1, y_true, q10), axis=0)\n",
    "        pin90_h = tf.reduce_mean(quantile_loss(0.9, y_true, q90), axis=0)\n",
    "\n",
    "        # Construct weights\n",
    "        decay_factors = tf.pow(decay, tf.cast(tf.range(H - 1), tf.float32))\n",
    "        remaining_weights = second_weight * decay_factors\n",
    "        weights = tf.concat([[initial_weight], remaining_weights], axis=0)  # shape [H]\n",
    "\n",
    "        weighted_pinball = tf.reduce_sum(weights * (pin10_h + pin90_h))\n",
    "        normalization = tf.reduce_sum(weights)\n",
    "        return 50*weighted_pinball / normalization\n",
    "\n",
    "    return Q_metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ccbc53-89c0-450a-9594-841dc1b53125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StepDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_lr, drop_every_x_steps):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.drop_every_x_steps = drop_every_x_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        factor = tf.math.floor(step / self.drop_every_x_steps)\n",
    "        return self.initial_lr * tf.math.pow(0.3, factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b0df69-30c0-4a38-82ef-ad335310b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_quantile_loss_factory(\n",
    "    H, lambda_mean=1.0, lambda_q=1.0, lambda_width=0.0,\n",
    "    initial_weight=1.0, second_weight=1.0, decay=0.7):\n",
    "    \"\"\"\n",
    "    Returns a loss(y_true, y_pred) that:\n",
    "      - Combines MSE, quantile loss, and optional interval width penalty.\n",
    "      - Applies initial_weight to horizon 0,\n",
    "      - Applies second_weight * decay^(i-1) to horizons i >= 1.\n",
    "    \"\"\"\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        q10, mu, q90 = tf.unstack(y_pred, axis=-1)  # [batch, H]\n",
    "\n",
    "        # Per-horizon losses\n",
    "        mse_h    = tf.reduce_mean(tf.square((y_true - mu) * 30), axis=0)\n",
    "        pin10_h  = tf.reduce_mean(quantile_loss(0.1, y_true, q10), axis=0)\n",
    "        pin90_h  = tf.reduce_mean(quantile_loss(0.9, y_true, q90), axis=0)\n",
    "        width_h  = tf.reduce_mean(q90 - q10, axis=0)\n",
    "\n",
    "        # Construct weights: [initial_weight, second_weight * decay^0, ..., decay^{H-2}]\n",
    "        decay_factors = tf.pow(decay, tf.cast(tf.range(H - 1), tf.float32))  # [H-1]\n",
    "        remaining_weights = second_weight * decay_factors\n",
    "        weights = tf.concat([[initial_weight], remaining_weights], axis=0)  # shape [H]\n",
    "\n",
    "        # Weighted losses\n",
    "        weighted_mse = tf.reduce_sum(weights * mse_h)\n",
    "        weighted_pinball = tf.reduce_sum(weights * (pin10_h + pin90_h))\n",
    "        weighted_width = tf.reduce_sum(weights * width_h)\n",
    "\n",
    "        total_loss = (\n",
    "            lambda_mean * weighted_mse +\n",
    "            lambda_q * weighted_pinball +\n",
    "            lambda_width * weighted_width\n",
    "        )\n",
    "        normalization = tf.reduce_sum(weights)\n",
    "        return total_loss / normalization\n",
    "\n",
    "    return loss_fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e1e1f7-e8c6-4b4d-98a5-472ce5eeb4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(q, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Pinball loss per element: L = max(q * e, (q-1) * e) where e = y_true - y_pred.\n",
    "    Returns shape [batch, H].\n",
    "    \"\"\"\n",
    "    e = y_true - y_pred\n",
    "    return tf.maximum(q * e, (q - 1.0) * e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9a477-ca3f-4cad-8706-aa7870e329cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_num_images=num_previous_images+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3204656-d7b5-41f6-b926-fe1393c0f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_train_model(\n",
    "    merged_data_dict,\n",
    "    image_dict,\n",
    "    batch_size,\n",
    "    num_previous_images,\n",
    "    Hdelta_t,\n",
    "    expected_num_images,\n",
    "    epochs=30,\n",
    "    use_ghi_now=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a quantile model using 20% random subsets of the training data at each epoch,\n",
    "    holds out 10% for validation and 10% for final testing. Prints per-horizon RMSE,\n",
    "    plots forecast skill, and returns the trained model and history_all.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Prepare keys, skipping initial/history and final horizon\n",
    "    all_keys = sorted(merged_data_dict.keys())\n",
    "    start = num_previous_images\n",
    "    end = -Hdelta_t\n",
    "    keys = np.array(all_keys)[start:end]\n",
    "\n",
    "    # initial split: 80% train_val, 20% temp (to split into 10% val, 10% test)\n",
    "    train_val_keys, temp_keys = train_test_split(\n",
    "        keys, test_size=0.2, shuffle=True, random_state=42\n",
    "    )\n",
    "    # split temp into validation and test (each 10% of total)\n",
    "    val_keys, test_keys = train_test_split(\n",
    "        temp_keys, test_size=0.5, shuffle=True, random_state=42\n",
    "    )\n",
    "\n",
    "    # prepare static datasets for validation and testing\n",
    "    val_ds = solcast_batchloader(\n",
    "        val_keys, merged_data_dict, image_dict,\n",
    "        batch_size, expected_num_images, Hdelta_t, use_ghi_now\n",
    "    )\n",
    "    test_ds = solcast_batchloader(\n",
    "        test_keys, merged_data_dict, image_dict,\n",
    "        batch_size, expected_num_images, Hdelta_t, use_ghi_now\n",
    "    )\n",
    "\n",
    "    # build model\n",
    "    model = create_model_quantiles_with_ghi_and_solcast(\n",
    "        num_previous_images, Hdelta_t\n",
    "    )\n",
    "\n",
    "    # Initialize history_all dictionary\n",
    "    history_all = {}\n",
    "\n",
    "    # training loop: each epoch uses fresh 20% sample of train_val keys\n",
    "    for epoch in range(epochs):\n",
    "        # sample 20% of train_val_keys\n",
    "        k = int(0.1 * len(train_val_keys))\n",
    "        sub_keys = np.random.choice(train_val_keys, size=k, replace=False)\n",
    "\n",
    "        # build train dataset for this epoch\n",
    "        train_ds = solcast_batchloader(\n",
    "            sub_keys, merged_data_dict, image_dict,\n",
    "            batch_size, expected_num_images, Hdelta_t, use_ghi_now\n",
    "        )\n",
    "        steps_per_epoch = train_ds.cardinality().numpy()\n",
    "        val_steps = val_ds.cardinality().numpy()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: training on {k} samples\")\n",
    "        hist = model.fit(\n",
    "            train_ds,\n",
    "            epochs=epoch+1,\n",
    "            initial_epoch=epoch,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=val_ds,\n",
    "            validation_steps=val_steps,\n",
    "            verbose=2,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        # Accumulate history\n",
    "        for key, values in hist.history.items():\n",
    "            history_all.setdefault(key, []).extend(values)\n",
    "\n",
    "    # final evaluation on test set\n",
    "    y_trues, y_q10s, y_means, y_q90s = [], [], [], []\n",
    "    for x_batch, y_batch in test_ds:\n",
    "        y_trues.append(y_batch.numpy())\n",
    "        preds = model.predict(x_batch, verbose=0)\n",
    "        y_q10s.append(preds[..., 0])\n",
    "        y_means.append(preds[..., 1])\n",
    "        y_q90s.append(preds[..., 2])\n",
    "\n",
    "    y_true = np.vstack(y_trues)\n",
    "    y_q10 = np.vstack(y_q10s)\n",
    "    y_mean = np.vstack(y_means)\n",
    "    y_q90 = np.vstack(y_q90s)\n",
    "\n",
    "    # scale back using max clear-sky\n",
    "    max_factors = np.array([merged_data_dict[k]['max_clear_sky_ghi'] for k in test_keys])\n",
    "    max_factors = np.repeat(max_factors[:, None], Hdelta_t+1, axis=1)\n",
    "    y_true_real = y_true * max_factors[:len(y_true)]\n",
    "    y_mean_real = y_mean * max_factors[:len(y_true)]\n",
    "\n",
    "    # compute and print RMSE per horizon\n",
    "    rmse = np.sqrt(np.mean((y_true_real - y_mean_real)**2, axis=0))\n",
    "    print(\"\\nTest RMSE per horizon:\")\n",
    "    for i, v in enumerate(rmse):\n",
    "        print(f\"  Horizon {i}: RMSE = {v:.3f}\")\n",
    "\n",
    "    # plot forecast skill for horizons 1..Hdelta_t\n",
    "    forecast_skill_plot(merged_data_dict, test_keys, Hdelta_t, rmse[1:])\n",
    "\n",
    "    return model, history_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcbc87a-b1f8-404a-9207-3f0f60270fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model,history=short_train_model(\n",
    "    merged_data_dict,\n",
    "    image_dict,\n",
    "    16,\n",
    "    num_previous_images,\n",
    "    Hdelta_t,\n",
    "    expected_num_images,\n",
    "    epochs=60,\n",
    "    use_ghi_now=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebbfb03-b1d2-47a7-816f-7205cd36702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate=0.1\n",
    "l2_lambda=1e-5\n",
    "lambda_mean_loss=1.0\n",
    "lambda_quantile_loss=150.0\n",
    "lambda_width_loss=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b76d4c-532f-4158-9e0d-d9971d0696cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.AdamW()\n",
    "H = Hdelta_t + 1\n",
    "\n",
    "loss_fn = combined_quantile_loss_factory(\n",
    "    H, lambda_mean=lambda_mean_loss, lambda_q=lambda_quantile_loss, lambda_width=lambda_width_loss\n",
    ")\n",
    "weighted_mse_metric = weighted_compressed_mse(\n",
    "    H, initial_weight=1.0, second_weight=1.0, decay=0.5\n",
    ")\n",
    "weighted_quantile_metric = weighted_quantile_loss(\n",
    "    H, initial_weight=1.0, second_weight=1.0, decay=0.5\n",
    ")\n",
    "def mean_prediction_interval_width(y_true, y_pred):\n",
    "    q10, _, q90 = tf.unstack(y_pred, axis=-1)\n",
    "    width = q90 - q10\n",
    "    return tf.reduce_mean(width)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    metrics=[\n",
    "        mean_channel_mae,\n",
    "        weighted_mse_metric,\n",
    "        weighted_quantile_metric,\n",
    "        mean_prediction_interval_width\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6698f8d0-c787-4163-a8d1-4672d7b85b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose `model` is your trained tf.keras.Model\n",
    "model.save('with_solcast_model.h5')  \n",
    "# This will create a SavedModel directory at './my_quantile_model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e48d02e-3415-4600-9344-1e433c72442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "def Q_short_cross_validate_model_withGHI_and_solcast(\n",
    "    merged_data_dict,\n",
    "    image_dict,\n",
    "    batch_size,\n",
    "    num_previous_images,\n",
    "    Hdelta_t,\n",
    "    expected_num_images,\n",
    "    x_splits=5,\n",
    "    epochs=30,\n",
    "    train_fraction=0.3,\n",
    "    use_ghi_now=True\n",
    "):\n",
    "    all_keys = sorted(merged_data_dict.keys())\n",
    "    start = num_previous_images * delta_t\n",
    "    end = -Hdelta_t * delta_t\n",
    "    keys = np.array(all_keys)[start:end]\n",
    "    split_size = len(keys) // x_splits\n",
    "\n",
    "    skill_splits       = []\n",
    "    picp_model_splits  = []\n",
    "    pinaw_model_splits = []\n",
    "    legend_labels      = []\n",
    "\n",
    "    for split_idx in range(x_splits):\n",
    "        print(f\"\\n=== Split {split_idx+1}/{x_splits} ===\")\n",
    "        test_keys  = keys[split_idx*split_size:(split_idx+1)*split_size]\n",
    "        train_keys = np.setdiff1d(keys, test_keys)\n",
    "\n",
    "        val_n    = int(0.1 * len(train_keys))\n",
    "        val_keys = train_keys[:val_n]\n",
    "        tr_keys  = train_keys[val_n:]\n",
    "\n",
    "        val_ds  = solcast_batchloader(val_keys, merged_data_dict, image_dict,\n",
    "                                      batch_size, expected_num_images, Hdelta_t, use_ghi_now)\n",
    "        test_ds = solcast_batchloader(test_keys, merged_data_dict, image_dict,\n",
    "                                      batch_size, expected_num_images, Hdelta_t, use_ghi_now)\n",
    "\n",
    "        model = create_model_quantiles_with_ghi_and_solcast(num_previous_images, Hdelta_t)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            k  = int(train_fraction * len(tr_keys))\n",
    "            bs = np.random.choice(tr_keys, k, replace=False)\n",
    "            train_ds = solcast_batchloader(bs, merged_data_dict, image_dict,\n",
    "                                           batch_size, expected_num_images, Hdelta_t, use_ghi_now)\n",
    "\n",
    "            model.fit(train_ds,\n",
    "                      epochs=epoch+1, initial_epoch=epoch,\n",
    "                      steps_per_epoch=train_ds.cardinality().numpy(),\n",
    "                      validation_data=val_ds,\n",
    "                      validation_steps=val_ds.cardinality().numpy(),\n",
    "                      verbose=2, shuffle=False)\n",
    "\n",
    "        y_true_blocks, q10_blocks, q50_blocks, q90_blocks = [], [], [], []\n",
    "        timestamps = []\n",
    "        for x_b, y_b in test_ds:\n",
    "            preds = model.predict(x_b, verbose=0)\n",
    "            y_true_blocks.append(y_b.numpy())\n",
    "            q10_blocks.append(preds[..., 0])\n",
    "            q50_blocks.append(preds[..., 1])\n",
    "            q90_blocks.append(preds[..., 2])\n",
    "        y_true = np.vstack(y_true_blocks)\n",
    "        q10    = np.vstack(q10_blocks)\n",
    "        q50    = np.vstack(q50_blocks)\n",
    "        q90    = np.vstack(q90_blocks)\n",
    "\n",
    "        for k in test_keys:\n",
    "            timestamps.append(merged_data_dict[k]['key_timestamp'])\n",
    "        timestamps = pd.to_datetime(timestamps)[:y_true.shape[0]]\n",
    "        split_start = timestamps.min().strftime('%Y-%m-%d')\n",
    "        split_end   = timestamps.max().strftime('%Y-%m-%d')\n",
    "        legend_labels.append(f\"Split {split_idx+1}: {split_start} → {split_end}\")\n",
    "\n",
    "        maxfs = np.array([merged_data_dict[k]['max_clear_sky_ghi'] for k in test_keys])\n",
    "        maxfs = np.repeat(maxfs[:, None], Hdelta_t+1, axis=1)[:y_true.shape[0]]\n",
    "        y_true_r = y_true * maxfs\n",
    "        q10_r    = q10    * maxfs\n",
    "        q50_r    = q50    * maxfs\n",
    "        q90_r    = q90    * maxfs\n",
    "\n",
    "        rmse_model = np.sqrt(np.mean((y_true_r - q50_r)**2, axis=0))\n",
    "        print(f\"→ RMSE model: {np.round(rmse_model, 2)}\")\n",
    "\n",
    "        # Forecast skill vs Solcast (optional: replace with Solcast RMSE here)\n",
    "        skill = [1 - rmse_model[h] / rmse_model[h] for h in range(1, Hdelta_t+1)]  # dummy comparison\n",
    "        skill_splits.append(skill)\n",
    "\n",
    "        # PICP and PINAW\n",
    "        yt = y_true_r[:, 1:]\n",
    "        Lm = q10_r[:, 1:]\n",
    "        Um = q90_r[:, 1:]\n",
    "        Mf = maxfs[:, 1:]\n",
    "\n",
    "        picp_m  = ((yt >= Lm) & (yt <= Um)).mean(axis=0)\n",
    "        pinaw_m = ((Um - Lm) / Mf).mean(axis=0)\n",
    "\n",
    "        picp_model_splits.append(picp_m)\n",
    "        pinaw_model_splits.append(pinaw_m)\n",
    "\n",
    "    # --- PLOTTING ---\n",
    "    plt.figure(figsize=(9,6))\n",
    "    for i, skill in enumerate(skill_splits):\n",
    "        plt.plot([(h+1)*delta_t for h in range(len(skill))], skill,\n",
    "                 marker='o', label=legend_labels[i])\n",
    "    plt.axhline(0, color='r', linestyle='--')\n",
    "    plt.xlabel(\"Horizon (min)\")\n",
    "    plt.ylabel(\"Forecast Skill\")\n",
    "    plt.title(\"Forecast Skill vs Horizon\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('q_ghi_solcast_skill.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(9,6))\n",
    "    for i, picp in enumerate(picp_model_splits):\n",
    "        plt.plot([(h+1)*delta_t for h in range(len(picp))], picp,\n",
    "                 marker='o', label=legend_labels[i])\n",
    "    plt.xlabel(\"Horizon (min)\")\n",
    "    plt.ylabel(\"PICP (Model)\")\n",
    "    plt.title(\"PICP across horizons\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig('q_ghi_solcast_picp.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(9,6))\n",
    "    for i, pinaw in enumerate(pinaw_model_splits):\n",
    "        plt.plot([(h+1)*delta_t for h in range(len(pinaw))], pinaw,\n",
    "                 marker='o', label=legend_labels[i])\n",
    "    plt.xlabel(\"Horizon (min)\")\n",
    "    plt.ylabel(\"PINAW (Model)\")\n",
    "    plt.title(\"PINAW across horizons\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig('q_ghi_solcast_pinaw.png')\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"skill\": skill_splits,\n",
    "        \"picp_model\": picp_model_splits,\n",
    "        \"pinaw_model\": pinaw_model_splits\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e982d-6c2c-4e5d-b478-52f1c1154f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_model_quantiles_with_ghi_and_solcast(\n",
    "    num_previous_images,\n",
    "    Hdelta_t,\n",
    "    dropout_rate=0.1,\n",
    "    l2_lambda=1e-5,\n",
    "    lambda_mean_loss=1.0,\n",
    "    lambda_quantile_loss=150.0,\n",
    "    lambda_width_loss=5\n",
    "):\n",
    "    H = Hdelta_t + 1\n",
    "\n",
    "    # --- inputs ---\n",
    "    image_input = layers.Input(shape=(num_previous_images + 1, 250, 250, 3), name='image_sequence')\n",
    "    clear_sky_input = layers.Input(shape=(H,), name='clear_sky_input')\n",
    "    ghi_now_input = layers.Input(shape=(1,), name='ghi_now_input')\n",
    "    solcast_input = layers.Input(shape=(3, Hdelta_t), name='solcast_input')  # [3, Hdelta_t]\n",
    "\n",
    "    # --- deeper 3D-CNN backbone ---\n",
    "    x = layers.Conv3D(32, (3, 3, 3), padding='same', activation='relu')(image_input)\n",
    "    x = layers.Conv3D(32, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling3D((1, 2, 2))(x)\n",
    "\n",
    "    x = layers.Conv3D(64, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv3D(64, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling3D((1, 2, 2))(x)\n",
    "\n",
    "    # --- stacked ConvLSTM2D ---\n",
    "    x = layers.ConvLSTM2D(64, (3, 3), padding='same', return_sequences=True, activation='tanh')(x)\n",
    "    x = layers.ConvLSTM2D(64, (3, 3), padding='same', return_sequences=False, activation='tanh')(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.SpatialDropout2D(dropout_rate)(x)\n",
    "\n",
    "    # --- spatial pooling ---\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # --- deeper clear-sky feature embedding ---\n",
    "    cs = layers.Dense(128, activation='relu')(clear_sky_input)\n",
    "    cs = layers.Dropout(dropout_rate)(cs)\n",
    "    cs = layers.Dense(64, activation='relu')(cs)\n",
    "    cs = layers.Dense(32, activation='relu')(cs)\n",
    "\n",
    "    # --- solcast embedding ---\n",
    "    s = layers.Flatten()(solcast_input)  # flatten [3, Hdelta_t]\n",
    "    s = layers.Dense(64, activation='relu')(s)\n",
    "    s = layers.Dropout(dropout_rate)(s)\n",
    "    s = layers.Dense(32, activation='relu')(s)\n",
    "\n",
    "    # --- concatenate all features ---\n",
    "    x = layers.Concatenate()([x, cs, ghi_now_input, s])\n",
    "\n",
    "    # --- MLP head with residual connection ---\n",
    "    mlp_input = x\n",
    "    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda))(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "    mlp_proj = layers.Dense(128)(mlp_input)  # Project mlp_input to match x\n",
    "    x = layers.Add()([x, mlp_proj])  # Residual connection\n",
    "\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "    # --- output: [H, 3] for [q10, mean, q90] per horizon ---\n",
    "    head = layers.Dense(3 * H, activation='linear')(x)\n",
    "    output = layers.Reshape((H, 3), name='quantiles')(head)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[image_input, clear_sky_input, ghi_now_input, solcast_input],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "    # --- losses and optimizer ---\n",
    "    weighted_mse_metric = weighted_compressed_mse(\n",
    "        H, initial_weight=1.0, second_weight=1.0, decay=0.5\n",
    "    )\n",
    "    weighted_quantile_metric = weighted_quantile_loss(\n",
    "        H, initial_weight=1.0, second_weight=1.0, decay=0.5\n",
    "    )\n",
    "\n",
    "    initial_lr = 3e-4\n",
    "    drop_every_x_steps = 210 * 5\n",
    "    lr_schedule = StepDecay(initial_lr, drop_every_x_steps)\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_schedule)\n",
    "\n",
    "    loss_fn = combined_quantile_loss_factory(\n",
    "        H, lambda_mean=lambda_mean_loss, lambda_q=lambda_quantile_loss, lambda_width=lambda_width_loss\n",
    "    )\n",
    "\n",
    "    def mean_prediction_interval_width(y_true, y_pred):\n",
    "        q10, _, q90 = tf.unstack(y_pred, axis=-1)\n",
    "        width = q90 - q10\n",
    "        return tf.reduce_mean(width)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_fn,\n",
    "        metrics=[\n",
    "            mean_channel_mae,\n",
    "            weighted_mse_metric,\n",
    "            weighted_quantile_metric,\n",
    "            mean_prediction_interval_width\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f55f459-27ca-46db-8beb-00b8e3b4b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_short_cross_validate_model_withGHI_and_solcast(\n",
    "    merged_data_dict,\n",
    "    image_dict,\n",
    "    32,\n",
    "    num_previous_images,\n",
    "    Hdelta_t,\n",
    "    expected_num_images,\n",
    "    x_splits=5,\n",
    "    epochs=1,\n",
    "    train_fraction=0.1,\n",
    "    use_ghi_now=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f6a595-8fa7-456d-9bb0-c70963fea257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "keys = list(merged_data_dict.keys())\n",
    "# infer Hdelta_t from lengths\n",
    "Hdelta_t = len(merged_data_dict[keys[0]]['solcast_predictions'][0])\n",
    "\n",
    "picp = np.zeros(Hdelta_t)\n",
    "\n",
    "for h in range(Hdelta_t):\n",
    "    hits = []\n",
    "    for k in keys:\n",
    "        sol_preds = merged_data_dict[k]['solcast_predictions']  \n",
    "        # sol_preds[0] = 10th, [1]=mean, [2]=90th, each a list of length Hdelta_t\n",
    "        sol10  = sol_preds[0][h]\n",
    "        sol50  = sol_preds[1][h]\n",
    "        sol90  = sol_preds[2][h]\n",
    "        # true normalized GHI at horizon h+1\n",
    "        y_true_norm = merged_data_dict[k]['ghi_values_normalized'][h+1]\n",
    "        hits.append((y_true_norm >= sol10) and (y_true_norm <= sol90))\n",
    "    picp[h] = np.mean(hits)\n",
    "\n",
    "# Print per-horizon PICP\n",
    "for h, p in enumerate(picp, start=1):\n",
    "    print(f\"Horizon {h*delta_t} min: PICP = {p:.3f}\")\n",
    "\n",
    "print(f\"\\nOverall PICP (80% interval): {picp.mean():.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6a7f49-35dd-445c-80c6-004d815c76db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
