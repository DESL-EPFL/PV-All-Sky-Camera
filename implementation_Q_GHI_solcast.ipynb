{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2bd9f3-721d-4b9b-bd21-f9ddcbcb9e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  6.51it/s]\n",
      "WARNING:absl:Skipping variable loading for optimizer 'AdamW', because it has 1 variables whereas the saved optimizer has 2 variables. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "weights laoded\n",
      "Max clear-sky GHI on 2025-05-30 = 874.3637293802624\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "[[[586.18787 650.4661  661.3385 ]\n",
      "  [496.1683  502.7906  573.24054]\n",
      "  [485.06497 483.52374 439.4149 ]\n",
      "  [424.19778 469.70602 535.37195]\n",
      "  [419.89423 446.34555 429.36725]\n",
      "  [477.57388 466.77856 495.04813]\n",
      "  [403.11954 383.32886 516.2467 ]\n",
      "  [387.20074 498.72586 455.66193]\n",
      "  [393.55914 450.21808 548.3646 ]\n",
      "  [336.54318 486.9724  445.3326 ]\n",
      "  [284.6566  432.04446 495.43982]\n",
      "  [296.68658 396.915   491.57385]\n",
      "  [313.0871  343.2408  506.67694]\n",
      "  [237.14198 350.07175 385.36206]\n",
      "  [246.65596 400.09018 511.0394 ]\n",
      "  [277.89508 301.26877 541.0569 ]\n",
      "  [170.12335 320.14862 443.42984]]]\n",
      "InfluxDB connection is healthy: True\n",
      "1748595064271418112\n",
      "586.18787 0\n",
      "650.4661 0\n",
      "661.3385 0\n",
      "496.1683 15\n",
      "502.7906 15\n",
      "573.24054 15\n",
      "485.06497 30\n",
      "483.52374 30\n",
      "439.4149 30\n",
      "424.19778 45\n",
      "469.70602 45\n",
      "535.37195 45\n",
      "419.89423 60\n",
      "446.34555 60\n",
      "429.36725 60\n",
      "477.57388 75\n",
      "466.77856 75\n",
      "495.04813 75\n",
      "403.11954 90\n",
      "383.32886 90\n",
      "516.2467 90\n",
      "387.20074 105\n",
      "498.72586 105\n",
      "455.66193 105\n",
      "393.55914 120\n",
      "450.21808 120\n",
      "548.3646 120\n",
      "336.54318 135\n",
      "486.9724 135\n",
      "445.3326 135\n",
      "284.6566 150\n",
      "432.04446 150\n",
      "495.43982 150\n",
      "296.68658 165\n",
      "396.915 165\n",
      "491.57385 165\n",
      "313.0871 180\n",
      "343.2408 180\n",
      "506.67694 180\n",
      "237.14198 195\n",
      "350.07175 195\n",
      "385.36206 195\n",
      "246.65596 210\n",
      "400.09018 210\n",
      "511.0394 210\n",
      "277.89508 225\n",
      "301.26877 225\n",
      "541.0569 225\n",
      "170.12335 240\n",
      "320.14862 240\n",
      "443.42984 240\n",
      "[[[586.18787 650.4661  661.3385 ]\n",
      "  [496.1683  502.7906  573.24054]\n",
      "  [485.06497 483.52374 439.4149 ]\n",
      "  [424.19778 469.70602 535.37195]\n",
      "  [419.89423 446.34555 429.36725]\n",
      "  [477.57388 466.77856 495.04813]\n",
      "  [403.11954 383.32886 516.2467 ]\n",
      "  [387.20074 498.72586 455.66193]\n",
      "  [393.55914 450.21808 548.3646 ]\n",
      "  [336.54318 486.9724  445.3326 ]\n",
      "  [284.6566  432.04446 495.43982]\n",
      "  [296.68658 396.915   491.57385]\n",
      "  [313.0871  343.2408  506.67694]\n",
      "  [237.14198 350.07175 385.36206]\n",
      "  [246.65596 400.09018 511.0394 ]\n",
      "  [277.89508 301.26877 541.0569 ]\n",
      "  [170.12335 320.14862 443.42984]]]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "# Now you can safely import other libraries after installation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import boto3\n",
    "import cv2\n",
    "import pvlib\n",
    "from pvlib import location\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "latitude = 46.518404934146574\n",
    "longitude = 6.565174801663707\n",
    "batiment = location.Location(latitude, longitude, tz='UTC', altitude=400, name='ELD')\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import bisect\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import plotly.graph_objects as go\n",
    "import h5py\n",
    "from sklearn.linear_model import Ridge, ElasticNet, LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import random as python_random\n",
    "from influxdb_client import InfluxDBClient\n",
    "np.random.seed(12345)\n",
    "python_random.seed(12345)\n",
    "access_key = '***'\n",
    "secret_key = '***'\n",
    "bucket_id  = '***'\n",
    "endpoint = '***'\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key,\n",
    "    endpoint_url=endpoint\n",
    ")\n",
    "\n",
    "\n",
    "#the model was trained using 2 images, with 5 min difference\n",
    "\n",
    "\n",
    "def apply_circular_mask(image):\n",
    "    center=(640, 512)\n",
    "    radius=535\n",
    "    # Create a mask with the same shape as the image\n",
    "    mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "    # Draw a filled white circle on the mask\n",
    "    cv2.circle(mask, center, radius, (255, 255, 255), -1)\n",
    "    # Apply the mask to the image\n",
    "    masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "    return masked_image\n",
    "\n",
    "\n",
    "def correct_fisheye_full_view(input_image):\n",
    "    \"\"\"\n",
    "    Corrects fisheye distortion for a given image and preserves the whole image.\n",
    "    \"\"\"\n",
    "    # Get the size of the input image\n",
    "    height, width = input_image.shape[:2]\n",
    "    # Define intrinsic camera matrix and distortion coefficients\n",
    "    k = np.array([[437.051833868485, 0, 610.832839633424],\n",
    "              [0, 442.486815359197, 496.451253984502],\n",
    "              [0, 0, 1]])\n",
    "    d = np.array([-0.146542838644292, 0.0114721530486779, 0, 0])\n",
    "\n",
    "    # Generate the new camera matrix based on the desired output view\n",
    "    new_camera_matrix, _ = cv2.getOptimalNewCameraMatrix(k, d, (width, height), 1, (width, height))\n",
    "\n",
    "    # Undistort the image\n",
    "    undistorted_image = cv2.undistort(input_image, k, d, None, new_camera_matrix)\n",
    "\n",
    "    return undistorted_image\n",
    "\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import cv2\n",
    "\n",
    "\n",
    "def download_and_process_image(key):\n",
    "    try:\n",
    "        # Descargar desde S3\n",
    "        response = s3.get_object(Bucket=bucket_id, Key=key)\n",
    "        img_data = response['Body'].read()\n",
    "        img_array = np.frombuffer(img_data, np.uint8)\n",
    "        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "\n",
    "        # Procesamiento directo (sin copia)\n",
    "        img = apply_circular_mask(img)\n",
    "        img = correct_fisheye_full_view(img)\n",
    "        img_resized = cv2.resize(img, (250, 250), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        return key, img_resized\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading/processing image {key}: {e}\")\n",
    "        return key, None\n",
    "\n",
    "def get_latest_images_from_s3(num_previous_images=2, image_delta_t=15):\n",
    "    global s3, bucket_id\n",
    "\n",
    "    access_key = '***'\n",
    "    secret_key = '***'\n",
    "    bucket_id  = '***'\n",
    "    endpoint = '***'\n",
    "    folder_name = 'All_sky_camera/'\n",
    "    header = 'mean'\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=access_key,\n",
    "        aws_secret_access_key=secret_key,\n",
    "        endpoint_url=endpoint\n",
    "    )\n",
    "\n",
    "    # Obtener claves\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    all_images = []\n",
    "    for page in paginator.paginate(Bucket=bucket_id, Prefix=folder_name + header):\n",
    "        all_images.extend(page.get('Contents', []))\n",
    "\n",
    "    image_keys = sorted([obj['Key'] for obj in all_images])\n",
    "\n",
    "    available_keys = []\n",
    "    for i in range(num_previous_images + 1):\n",
    "        index = -1 - i * image_delta_t-18915#aqui\n",
    "        if abs(index) <= len(image_keys):\n",
    "            available_keys.append(image_keys[index])\n",
    "\n",
    "    # Descargar y procesar imágenes\n",
    "    processed_images = {}\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        results = list(tqdm(executor.map(download_and_process_image, available_keys), total=len(available_keys)))\n",
    "\n",
    "    for key, proc_img in results:\n",
    "        if proc_img is not None:\n",
    "            processed_images[key] = proc_img\n",
    "\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_dict = get_latest_images_from_s3(num_previous_images=2, image_delta_t=5)\n",
    "# image_dict contains 3 entries spaced 15 minutes apart\n",
    "\n",
    "\n",
    "keys=list(image_dict.keys())\n",
    "\n",
    "def extract_timestamp_from_image(image_key):\n",
    "    \"\"\"\n",
    "    Extracts the timestamp from the image key (of the form 'All_sky_camera/meanYYYY-MM-DD HH:MM:SS.ssssss.png'),\n",
    "    and localizes it to the Europe/Zurich timezone.\n",
    "    \"\"\"\n",
    "    timestamp_str = image_key.split('/')[1][4:-4]  # Extract timestamp string\n",
    "    timestamp = pd.to_datetime(timestamp_str)\n",
    "    localized_timestamp = timestamp.tz_localize('Europe/Zurich')\n",
    "    localized_timestamp=localized_timestamp.tz_convert('UTC')\n",
    "    return localized_timestamp\n",
    "\n",
    "\n",
    "\n",
    "timestamp=extract_timestamp_from_image(keys[0])\n",
    "\n",
    "\n",
    "Hdelta_t=16\n",
    "delta_t=15\n",
    "\n",
    "\n",
    "\n",
    "future_timestamps = [timestamp + i * timedelta(minutes=delta_t) for i in range(Hdelta_t + 1)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#cs vector from timestamps\n",
    "cs=batiment.get_clearsky(pd.DatetimeIndex(future_timestamps)).ghi.values\n",
    "\n",
    "\n",
    "class InfluxDBQueryClient:\n",
    "    def __init__(self, ip, port, token, org):\n",
    "        self.url = f\"http://{ip}:{port}\"\n",
    "        self.token = token\n",
    "        self.org = org\n",
    "        self.timeout=5e4\n",
    "        self.client = InfluxDBClient(url=self.url, token=self.token, org=self.org,timeout=self.timeout)\n",
    "\n",
    "    def query_measured_ghi(self, start_time, end_time, resolution):\n",
    "        \"\"\"\n",
    "        Query the measured GHI from the InfluxDB database\n",
    "        :param start_time: datetime.datetime\n",
    "        :param end_time: datetime.datetime\n",
    "        :param resolution: str\n",
    "        :return: pd.DataFrame\n",
    "        \"\"\"\n",
    "        bucket = \"microgrid_ST\"\n",
    "\n",
    "        # Convert datetime to Unix timestamp\n",
    "        t0 = round(start_time.timestamp())\n",
    "        tf = round(end_time.timestamp())\n",
    "\n",
    "        # Prepare the Flux query\n",
    "        flux_query = f\"\"\"\n",
    "        from(bucket: \"{bucket}\")\n",
    "        |> range(start: {t0}, stop: {tf})\n",
    "          |> filter(fn: (r) => r[\"_measurement\"] == \"microgrid\")\n",
    "          |> filter(fn: (r) => r[\"Resource\"] == \"meteobox_roof\")\n",
    "          |> filter(fn: (r) => r[\"_field\"] == \"GHI\")\n",
    "          |> aggregateWindow(every: {resolution}, fn: mean, createEmpty: false)\n",
    "          |> yield(name: \"mean\")\n",
    "          |> pivot(rowKey:[\"_time\"], columnKey: [\"Resource\"], valueColumn: \"_value\")\n",
    "        \"\"\"\n",
    "\n",
    "        # Query the data\n",
    "        query_api = self.client.query_api()\n",
    "        measured_GHI_api15 = query_api.query_data_frame(org=self.org, query=flux_query)\n",
    "        return measured_GHI_api15[0]\n",
    "\n",
    "    def query_ghi_DA_forecast(self, start_time, end_time, resolution):\n",
    "        \"\"\"\n",
    "        Query the forecast GHI from solcast day ahead from the InfluxDB database\n",
    "        :param start_time: datetime.datetime\n",
    "        :param end_time: datetime.datetime\n",
    "        :param resolution: str\n",
    "        :return: pd.DataFrame\n",
    "        \"\"\"\n",
    "        bucket = \"logging\"\n",
    "\n",
    "        # Convert datetime to Unix timestamp\n",
    "        t0 = round(start_time.timestamp())\n",
    "        tf = round(end_time.timestamp())\n",
    "\n",
    "        # Prepare the Flux query\n",
    "        flux_query = f\"\"\"\n",
    "        from(bucket: \"{bucket}\")\n",
    "        |> range(start: {t0}, stop: {tf})\n",
    "          |> filter(fn: (r) => r[\"_measurement\"] == \"WAday1\")\n",
    "          |> filter(fn: (r) => r[\"_field\"] == \"value\")\n",
    "          |> filter(fn: (r) => r[\"solcast_weekahead\"] == \"GHI\" or r[\"solcast_weekahead\"] == \"GHI10\" or r[\"solcast_weekahead\"] == \"GHI90\" or r[\"solcast_weekahead\"] == \"airT\")\n",
    "          |> aggregateWindow(every: {resolution}, fn: mean, createEmpty: false)\n",
    "          |> yield(name: \"mean\")\n",
    "          |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "        \"\"\"\n",
    "\n",
    "        # Query the data\n",
    "        query_api = self.client.query_api()\n",
    "        ghi_DA_forecast = query_api.query_data_frame(org=self.org, query=flux_query)\n",
    "        ghi_DA_forecast = ghi_DA_forecast.pivot(index='_time', columns='solcast_weekahead', values='_value').reset_index()\n",
    "        ghi_DA_forecast = ghi_DA_forecast.rename(columns={\"_time\":'time'}).set_index('time')\n",
    "        ghi_DA_forecast.columns.name = None\n",
    "        return ghi_DA_forecast\n",
    "    \n",
    "    \n",
    "    def query_nissan_leaf_voltage(self, start_time, end_time, resolution):\n",
    "            \"\"\"\n",
    "            Query the CellVoltage of NissanLeaf from the InfluxDB database\n",
    "            :param start_time: datetime.datetime\n",
    "            :param end_time: datetime.datetime\n",
    "            :param resolution: str\n",
    "            :return: pd.DataFrame\n",
    "            \"\"\"\n",
    "            bucket = \"microgrid_ST\"\n",
    "            t0 = round(start_time.timestamp())\n",
    "            tf = round(end_time.timestamp())\n",
    "\n",
    "            flux_query = f\"\"\"\n",
    "            from(bucket: \"microgrid_ST\")\n",
    "            |> range(start: {t0}, stop: {tf})\n",
    "            |> filter(fn: (r) => r[\"_measurement\"] == \"microgrid\")\n",
    "            |> filter(fn: (r) => r[\"Resource\"] == \"NissanLeaf\")\n",
    "            |> filter(fn: (r) => r[\"_field\"] == \"CellVoltage\")\n",
    "            |> aggregateWindow(every: {resolution}, fn: mean, createEmpty: false)\n",
    "            |> yield(name: \"mean\")\n",
    "            \"\"\"\n",
    "\n",
    "            query_api = self.client.query_api()\n",
    "            nissan_leaf_voltage = query_api.query_data_frame(org=self.org, query=flux_query)\n",
    "            nissan_leaf_voltage = nissan_leaf_voltage.pivot(index='_time', columns='Cell', values='_value').reset_index()\n",
    "            return nissan_leaf_voltage \n",
    "\n",
    "\n",
    "    def pull_solcast_forecast(self, forecast_time):\n",
    "        \"\"\"\n",
    "        Pull Solcast forecast data for GHI from InfluxDB.\n",
    "        :param forecast_time: datetime.datetime, start of the query range\n",
    "        :return: pd.DataFrame\n",
    "        \"\"\"\n",
    "        to = forecast_time - timedelta(minutes=14)\n",
    "        # Convert datetime to Unix timestamp\n",
    "        tf = round(forecast_time.timestamp())\n",
    "        t0 = round(to.timestamp())\n",
    "\n",
    "        bucket = \"Forecasting_ST\"\n",
    "\n",
    "        # Prepare the Flux query\n",
    "        flux_query = f\"\"\"\n",
    "        from(bucket: \"{bucket}\")\n",
    "        |> range(start: {t0}, stop: {tf})\n",
    "        |> filter(fn: (r) => r[\"type\"] == \"Solcast\")\n",
    "        |> filter(fn: (r) => r[\"_field\"] == \"ghi\")\n",
    "        |> pivot(rowKey:[\"_time\"], columnKey: [\"prediction_time\"], valueColumn: \"_value\")\n",
    "        |> yield(name: \"mean\")\n",
    "        \"\"\"\n",
    "        # Query the data\n",
    "        query_api = self.client.query_api()\n",
    "        solcast_forecast = query_api.query_data_frame(org=self.org, query=flux_query)\n",
    "\n",
    "        return solcast_forecast\n",
    "    \n",
    "    def pull_solcast_forecast_bulk(self, start_time, end_time):\n",
    "        \"\"\"\n",
    "        Pull Solcast forecast data for GHI from InfluxDB over a time range.\n",
    "        :param start_time: datetime.datetime\n",
    "        :param end_time: datetime.datetime\n",
    "        :return: pd.DataFrame\n",
    "        \"\"\"\n",
    "        bucket = \"Forecasting_ST\"\n",
    "        t0 = round(start_time.timestamp())\n",
    "        tf = round(end_time.timestamp())\n",
    "    \n",
    "        flux_query = f\"\"\"\n",
    "        from(bucket: \"{bucket}\")\n",
    "        |> range(start: {t0}, stop: {tf})\n",
    "        |> filter(fn: (r) => r[\"type\"] == \"Solcast\")\n",
    "        |> filter(fn: (r) => r[\"_field\"] == \"ghi\")\n",
    "        |> pivot(rowKey:[\"_time\"], columnKey: [\"prediction_time\"], valueColumn: \"_value\")\n",
    "        |> yield(name: \"mean\")\n",
    "        \"\"\"\n",
    "    \n",
    "        query_api = self.client.query_api()\n",
    "        try:\n",
    "            df = query_api.query_data_frame(org=self.org, query=flux_query)\n",
    "        except Exception as e:\n",
    "            print(\"Error al hacer la consulta:\", e)\n",
    "            raise\n",
    "\n",
    "        #df = query_api.query_data_frame(org=self.org, query=flux_query)\n",
    "        return df\n",
    "    def pull_solcast_forecast_bulk2(self, start_time, end_time):\n",
    "        bucket = \"Forecasting_ST\"\n",
    "        t0 = round(start_time.timestamp())\n",
    "        tf = round(end_time.timestamp())\n",
    "    \n",
    "        flux_query = f\"\"\"\n",
    "        from(bucket: \"{bucket}\")\n",
    "        |> range(start: {t0}, stop: {tf})\n",
    "        |> filter(fn: (r) => r[\"type\"] == \"Solcast\")\n",
    "        |> filter(fn: (r) => r[\"_field\"] == \"ghi\" or r[\"_field\"] == \"ghi10\" or r[\"_field\"] == \"ghi90\")\n",
    "        |> pivot(rowKey:[\"_time\"], columnKey: [\"prediction_time\", \"_field\"], valueColumn: \"_value\")\n",
    "        |> yield(name: \"mean\")\n",
    "        \"\"\"\n",
    "    \n",
    "        query_api = self.client.query_api()\n",
    "        try:\n",
    "            df = query_api.query_data_frame(org=self.org, query=flux_query)\n",
    "        except Exception as e:\n",
    "            print(\"Error al hacer la consulta:\", e)\n",
    "            raise\n",
    "    \n",
    "        return df\n",
    "\n",
    "client = InfluxDBQueryClient(\n",
    "        ip='***',\n",
    "        port=***,\n",
    "        token=\"***\",\n",
    "        org=\"***\",\n",
    "    )\n",
    "max_attempts = 5  # Set your maximum number of retries\n",
    "attempts = 0\n",
    "\n",
    "while attempts < max_attempts:\n",
    "    try:\n",
    "        ghi_df = client.query_measured_ghi(future_timestamps[0] - timedelta(minutes=1), future_timestamps[0], '5m')\n",
    "        if ghi_df is not None and not ghi_df.empty:\n",
    "            value = ghi_df['_value'].iloc[0]\n",
    "            break\n",
    "        print('1')  # Optional debug message\n",
    "    except Exception:\n",
    "        print('2')  # Optional error message\n",
    "    attempts += 1\n",
    "    time.sleep(2)\n",
    "else:\n",
    "    raise RuntimeError(\"Maximum number of attempts reached without success.\")\n",
    "\n",
    "\n",
    "ghi_df\n",
    "\n",
    "\n",
    "solcast=client.pull_solcast_forecast_bulk2(timestamp-timedelta(minutes=15), timestamp)\n",
    "solcast_last = solcast.iloc[[-1]].reset_index(drop=True)\n",
    "\n",
    "\n",
    "lead_minutes = (np.arange(1, Hdelta_t+1) * 15.0).tolist()\n",
    "mean_cols = [f\"{m}_ghi\" for m in lead_minutes]\n",
    "q10_cols  = [f\"{m}_ghi10\" for m in lead_minutes]\n",
    "q90_cols  = [f\"{m}_ghi90\" for m in lead_minutes]\n",
    "\n",
    "solcast_means=solcast_last[mean_cols]\n",
    "solcast_q10=solcast_last[q10_cols]\n",
    "solcast_q90=solcast_last[q90_cols]\n",
    "\n",
    "\n",
    "def create_model_quantiles_with_ghi_and_solcast(\n",
    "    num_previous_images,\n",
    "    Hdelta_t,\n",
    "    dropout_rate=0.1,\n",
    "    l2_lambda=1e-5,\n",
    "    lambda_mean_loss=1.0,\n",
    "    lambda_quantile_loss=150.0,\n",
    "    lambda_width_loss=5\n",
    "):\n",
    "    H = Hdelta_t + 1\n",
    "\n",
    "    # --- inputs ---\n",
    "    image_input = layers.Input(shape=(num_previous_images + 1, 250, 250, 3), name='image_sequence')\n",
    "    clear_sky_input = layers.Input(shape=(H,), name='clear_sky_input')\n",
    "    ghi_now_input = layers.Input(shape=(1,), name='ghi_now_input')\n",
    "    solcast_input = layers.Input(shape=(3, Hdelta_t), name='solcast_input')  # [3, Hdelta_t]\n",
    "\n",
    "    # --- deeper 3D-CNN backbone ---\n",
    "    x = layers.Conv3D(32, (3, 3, 3), padding='same', activation='relu')(image_input)\n",
    "    x = layers.Conv3D(32, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling3D((1, 2, 2))(x)\n",
    "\n",
    "    x = layers.Conv3D(64, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv3D(64, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling3D((1, 2, 2))(x)\n",
    "\n",
    "    # --- stacked ConvLSTM2D ---\n",
    "    x = layers.ConvLSTM2D(64, (3, 3), padding='same', return_sequences=True, activation='tanh')(x)\n",
    "    x = layers.ConvLSTM2D(64, (3, 3), padding='same', return_sequences=False, activation='tanh')(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.SpatialDropout2D(dropout_rate)(x)\n",
    "\n",
    "    # --- spatial pooling ---\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # --- deeper clear-sky feature embedding ---\n",
    "    cs = layers.Dense(128, activation='relu')(clear_sky_input)\n",
    "    cs = layers.Dropout(dropout_rate)(cs)\n",
    "    cs = layers.Dense(64, activation='relu')(cs)\n",
    "    cs = layers.Dense(32, activation='relu')(cs)\n",
    "\n",
    "    # --- solcast embedding ---\n",
    "    s = layers.Flatten()(solcast_input)  # flatten [3, Hdelta_t]\n",
    "    s = layers.Dense(64, activation='relu')(s)\n",
    "    s = layers.Dropout(dropout_rate)(s)\n",
    "    s = layers.Dense(32, activation='relu')(s)\n",
    "\n",
    "    # --- concatenate all features ---\n",
    "    x = layers.Concatenate()([x, cs, ghi_now_input, s])\n",
    "\n",
    "    # --- MLP head with residual connection ---\n",
    "    mlp_input = x\n",
    "    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda))(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "    mlp_proj = layers.Dense(128)(mlp_input)  # Project mlp_input to match x\n",
    "    x = layers.Add()([x, mlp_proj])  # Residual connection\n",
    "\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "    # --- output: [H, 3] for [q10, mean, q90] per horizon ---\n",
    "    head = layers.Dense(3 * H, activation='linear')(x)\n",
    "    output = layers.Reshape((H, 3), name='quantiles')(head)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[image_input, clear_sky_input, ghi_now_input, solcast_input],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "    # --- losses and optimizer ---\n",
    "    weighted_mse_metric = weighted_compressed_mse(\n",
    "        H, initial_weight=1.0, second_weight=1.0, decay=0.5\n",
    "    )\n",
    "    weighted_quantile_metric = weighted_quantile_loss(\n",
    "        H, initial_weight=1.0, second_weight=1.0, decay=0.5\n",
    "    )\n",
    "\n",
    "    initial_lr = 3e-4\n",
    "    drop_every_x_steps = 210 * 5\n",
    "    lr_schedule = StepDecay(initial_lr, drop_every_x_steps)\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_schedule)\n",
    "\n",
    "    loss_fn = combined_quantile_loss_factory(\n",
    "        H, lambda_mean=lambda_mean_loss, lambda_q=lambda_quantile_loss, lambda_width=lambda_width_loss\n",
    "    )\n",
    "\n",
    "    def mean_prediction_interval_width(y_true, y_pred):\n",
    "        q10, _, q90 = tf.unstack(y_pred, axis=-1)\n",
    "        width = q90 - q10\n",
    "        return tf.reduce_mean(width)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_fn,\n",
    "        metrics=[\n",
    "            mean_channel_mae,\n",
    "            weighted_mse_metric,\n",
    "            weighted_quantile_metric,\n",
    "            mean_prediction_interval_width\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "class StepDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_lr, drop_every_x_steps):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.drop_every_x_steps = drop_every_x_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        factor = tf.math.floor(step / self.drop_every_x_steps)\n",
    "        return self.initial_lr * tf.math.pow(0.3, factor)\n",
    "def combined_quantile_loss_factory(\n",
    "    H, lambda_mean=1.0, lambda_q=1.0, lambda_width=0.0,\n",
    "    initial_weight=1.0, second_weight=1.0, decay=0.7):\n",
    "    \"\"\"\n",
    "    Returns a loss(y_true, y_pred) that:\n",
    "      - Combines MSE, quantile loss, and optional interval width penalty.\n",
    "      - Applies initial_weight to horizon 0,\n",
    "      - Applies second_weight * decay^(i-1) to horizons i >= 1.\n",
    "    \"\"\"\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        q10, mu, q90 = tf.unstack(y_pred, axis=-1)  # [batch, H]\n",
    "\n",
    "        # Per-horizon losses\n",
    "        mse_h    = tf.reduce_mean(tf.square((y_true - mu) * 30), axis=0)\n",
    "        pin10_h  = tf.reduce_mean(quantile_loss(0.1, y_true, q10), axis=0)\n",
    "        pin90_h  = tf.reduce_mean(quantile_loss(0.9, y_true, q90), axis=0)\n",
    "        width_h  = tf.reduce_mean(q90 - q10, axis=0)\n",
    "\n",
    "        # Construct weights: [initial_weight, second_weight * decay^0, ..., decay^{H-2}]\n",
    "        decay_factors = tf.pow(decay, tf.cast(tf.range(H - 1), tf.float32))  # [H-1]\n",
    "        remaining_weights = second_weight * decay_factors\n",
    "        weights = tf.concat([[initial_weight], remaining_weights], axis=0)  # shape [H]\n",
    "\n",
    "        # Weighted losses\n",
    "        weighted_mse = tf.reduce_sum(weights * mse_h)\n",
    "        weighted_pinball = tf.reduce_sum(weights * (pin10_h + pin90_h))\n",
    "        weighted_width = tf.reduce_sum(weights * width_h)\n",
    "\n",
    "        total_loss = (\n",
    "            lambda_mean * weighted_mse +\n",
    "            lambda_q * weighted_pinball +\n",
    "            lambda_width * weighted_width\n",
    "        )\n",
    "        normalization = tf.reduce_sum(weights)\n",
    "        return total_loss / normalization\n",
    "\n",
    "    return loss_fn\n",
    "# --- 3) custom MAE that only looks at the mean channel ---\n",
    "def mean_channel_mae(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_pred[...,1] is the mean prediction.\n",
    "    \"\"\"\n",
    "    mean_pred = y_pred[..., 1]\n",
    "    return tf.reduce_mean(tf.abs(y_true - mean_pred))\n",
    "def weighted_compressed_mse(H, initial_weight=1.0, second_weight=1.0, decay=0.5):\n",
    "    \"\"\"\n",
    "    Returns a metric function that computes a weighted, scaled MSE across horizons.\n",
    "    Weights:\n",
    "      - Horizon 0 → initial_weight\n",
    "      - Horizon i ≥ 1 → second_weight * decay^(i-1)\n",
    "    \"\"\"\n",
    "    def Mean_metric(y_true, y_pred):\n",
    "        mu = y_pred[..., 1]  # extract predicted mean, shape [batch, H]\n",
    "        error = (y_true - mu) * 30\n",
    "        mse_per_horizon = tf.reduce_mean(tf.square(erprintror), axis=0)  # shape [H]\n",
    "\n",
    "        # Compute weights\n",
    "        decay_factors = tf.pow(decay, tf.cast(tf.range(H - 1), tf.float32))\n",
    "        remaining_weights = second_weight * decay_factors\n",
    "        weights = tf.concat([[initial_weight], remaining_weights], axis=0)  # shape [H]\n",
    "\n",
    "        weighted_mse = tf.reduce_sum(weights * mse_per_horizon)\n",
    "        normalization = tf.reduce_sum(weights)\n",
    "        return weighted_mse / normalization\n",
    "\n",
    "    return Mean_metric\n",
    "def weighted_quantile_loss(H, initial_weight=1.0, second_weight=1.0, decay=0.5):\n",
    "    \"\"\"\n",
    "    Returns a metric function that computes the weighted average quantile loss\n",
    "    (pinball loss) over q10 and q90 across horizons.\n",
    "    \"\"\"\n",
    "    def Q_metric(y_true, y_pred):\n",
    "        q10 = y_pred[..., 0]\n",
    "        q90 = y_pred[..., 2]\n",
    "\n",
    "        # Pinball loss per horizon\n",
    "        pin10_h = tf.reduce_mean(quantile_loss(0.1, y_true, q10), axis=0)\n",
    "        pin90_h = tf.reduce_mean(quantile_loss(0.9, y_true, q90), axis=0)\n",
    "\n",
    "        # Construct weights\n",
    "        decay_factors = tf.pow(decay, tf.cast(tf.range(H - 1), tf.float32))\n",
    "        remaining_weights = second_weight * decay_factors\n",
    "        weights = tf.concat([[initial_weight], remaining_weights], axis=0)  # shape [H]\n",
    "\n",
    "        weighted_pinball = tf.reduce_sum(weights * (pin10_h + pin90_h))\n",
    "        normalization = tf.reduce_sum(weights)\n",
    "        return 50*weighted_pinball / normalization\n",
    "\n",
    "    return Q_metric\n",
    "\n",
    "\n",
    "num_previous_images=2\n",
    "Hdelta_t=16\n",
    "\n",
    "from tensorflow.keras import layers, Model, regularizers\n",
    "print('a')\n",
    "model=create_model_quantiles_with_ghi_and_solcast(\n",
    "    num_previous_images,\n",
    "    Hdelta_t)\n",
    "print('b')\n",
    "model.load_weights('***') \n",
    "\n",
    "print('weights laoded')\n",
    "\n",
    "#model = tf.keras.models.load_model('with_solcast_model.keras')\n",
    "\n",
    "\n",
    "images_array = np.stack([image_dict[k].astype(np.float32) / 255.0 for k in keys]) \n",
    "\n",
    "timestamp_day = pd.Timestamp(timestamp).normalize()\n",
    "\n",
    "day_times = pd.date_range(\n",
    "    start=timestamp_day,\n",
    "    end=timestamp_day + pd.Timedelta(days=1) - pd.Timedelta(seconds=1),\n",
    "    freq='15min'\n",
    ")\n",
    "\n",
    "cs_for_max = batiment.get_clearsky(day_times).ghi\n",
    "max_cs = cs_for_max.max()\n",
    "\n",
    "print(\"Max clear-sky GHI on\", timestamp_day.date(), \"=\", max_cs)\n",
    "\n",
    "solcast_q10\n",
    "\n",
    "solcast_q10 = solcast_q10.to_numpy().flatten()\n",
    "solcast_means = solcast_means.to_numpy().flatten()\n",
    "solcast_q90 = solcast_q90.to_numpy().flatten()\n",
    "\n",
    "\n",
    "images_input = np.expand_dims(images_array, axis=0)          # (1, 3, 250, 250, 3)\n",
    "clear_sky_input = np.expand_dims(cs, axis=0)/max_cs    # (1, 17)\n",
    "ghi_now_input = np.expand_dims(value, axis=0)/max_cs       # (1, 1)\n",
    "solcast_input=np.expand_dims(np.stack([solcast_q10, solcast_means, solcast_q90], axis=0),axis=0)/max_cs               #(1,3,Hdelta_t)\n",
    "# Diccionario de inputs\n",
    "inputs = {\n",
    "    'image_sequence': images_input,\n",
    "    'clear_sky_input': clear_sky_input,\n",
    "    'ghi_now_input': ghi_now_input,\n",
    "    'solcast_input': solcast_input\n",
    "}\n",
    "\n",
    "# Predicción\n",
    "prediction = model.predict(inputs)\n",
    "\n",
    "\n",
    "print(prediction*max_cs)\n",
    "\n",
    "real_forecast=prediction*max_cs\n",
    "\n",
    "bucket = \"***\"\n",
    "org = \"***\"\n",
    "token = \"***\"\n",
    "IP = '***'\n",
    "PORT = ***\n",
    "url = f\"http://{IP}:{PORT}\"\n",
    "client = InfluxDBClient(url=url, token=token, org=org)\n",
    "\n",
    "is_healthy = client.ping()\n",
    "print(\"InfluxDB connection is healthy:\", is_healthy)\n",
    "\n",
    "time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from influxdb_client import InfluxDBClient, Point\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "\n",
    "def log_dwd_influx( future_times,Hdelta_t,delta_t,real_forecast,location='EPFL'):\n",
    "    initial_time=future_times[0]\n",
    "\n",
    "    # InfluxDB connection details\n",
    "    bucket = \"***\"\n",
    "    org = \"***\"\n",
    "    token = \"***\"\n",
    "    IP = '***'\n",
    "    PORT = ***\n",
    "    url = f\"http://{IP}:{PORT}\"\n",
    "    client = InfluxDBClient(url=url, token=token, org=org)\n",
    "    write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "\n",
    "\n",
    "    # Get the time of the forecast\n",
    "    from zoneinfo import ZoneInfo\n",
    "    # specify the timezone of the prediction time, alreasy has\n",
    "    # Convert to nanoseconds since the UNIX epoch\n",
    "    timestamp_ns = int(initial_time.timestamp() * 1e9)\n",
    "    print(timestamp_ns)\n",
    "    # Get the fields we want to log\n",
    "   \n",
    "    fields = [\"GHI_10Q\", \"GHI_AVG\", \"GHI_90Q\"]\n",
    "    \n",
    "    # Create all the irradiance points to send to InfluxDB\n",
    "    points_irr = []\n",
    "    future_times=np.arange(0,(Hdelta_t+1)*delta_t,delta_t)\n",
    "    for t in range(len(future_times)):\n",
    "        for f in range(len(fields)):\n",
    "            prediction_time = future_times[t]\n",
    "            value = real_forecast[t][f]\n",
    "            print(value, str(prediction_time))\n",
    "            point_irr = Point(\"Forecast\")\\\n",
    "                .tag(\"type\", \"All_sky_camera\")\\\n",
    "                .tag(\"prediction_time\", str(prediction_time))\\\n",
    "                .tag(\"location\", location)\\\n",
    "                .field(fields[f], value)\\\n",
    "                .time(timestamp_ns )  # Use timestamp from array\n",
    "    \n",
    "            points_irr.append(point_irr)\n",
    "    #print(points_irr)\n",
    "    # Write log_dwd_influx(forecast_time,  prediction_times,location='EPFL')to InfluxDB\n",
    "    write_api.write(bucket=bucket, org=org, record=points_irr)\n",
    "\n",
    "  \n",
    "\n",
    "log_dwd_influx( future_timestamps,Hdelta_t,delta_t,real_forecast[0],location='EPFL')\n",
    "print(real_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa637a-2ff2-4d5d-bd3c-3c10abe78a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c807c4-7aa8-4241-a86b-447933cd556b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([420.33536927])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ghi_now_input*max_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7907b176-e2e0-49ae-9eb5-67fe0bc8f936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All_sky_camera/mean2025-05-30 10:51:04.271418.png',\n",
       " 'All_sky_camera/mean2025-05-30 10:46:05.187168.png',\n",
       " 'All_sky_camera/mean2025-05-30 10:41:05.021083.png']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66e9690a-876d-4dbe-9f69-27ce71a409c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.67041653, 0.7439308 , 0.7563654 ],\n",
       "        [0.56746215, 0.575036  , 0.6556088 ],\n",
       "        [0.5547634 , 0.5530007 , 0.5025539 ],\n",
       "        [0.48515028, 0.53719753, 0.6122989 ],\n",
       "        [0.48022833, 0.5104804 , 0.49106252],\n",
       "        [0.5461959 , 0.5338494 , 0.566181  ],\n",
       "        [0.4610433 , 0.43840894, 0.59042555],\n",
       "        [0.44283718, 0.5703872 , 0.52113545],\n",
       "        [0.45010918, 0.5149094 , 0.62715846],\n",
       "        [0.38490066, 0.5569449 , 0.5093219 ],\n",
       "        [0.32555854, 0.4941244 , 0.566629  ],\n",
       "        [0.3393171 , 0.45394725, 0.5622075 ],\n",
       "        [0.35807422, 0.3925607 , 0.57948077],\n",
       "        [0.27121663, 0.40037316, 0.4407343 ],\n",
       "        [0.28209767, 0.45757866, 0.58447003],\n",
       "        [0.3178255 , 0.34455773, 0.6188007 ],\n",
       "        [0.19456817, 0.3661504 , 0.50714576]]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ebc7d-7984-4c64-8198-22fbfd32290e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
